---
title: Automated visitor and wildlife monitoring with camera traps and machine learning
type: pub
encoding: UTF-8

---
<h1>Publication</h1>
<article id="csl-bib-container-RUAX49K6" class="csl-bib-container">
  <div class="csl-bib-body"> <div class="csl-entry">Mitterwallner, V., Peters, A., Edelhoff, H., Mathes, G., Nguyen, H., Peters, W., Heurich, M., &#38; Steinbauer, M. J. (2024). Automated visitor and wildlife monitoring with camera traps and machine learning. <i>Remote Sensing in Ecology and Conservation</i>, <i>10</i>(2), 236–247. <a href="https://doi.org/10.1002/rse2.367">https://doi.org/10.1002/rse2.367</a></div> </div>
  <div class="csl-bib-buttons">
    <a href="#taxonomy-article-RUAX49K6" alt="archive" class="csl-bib-button">Archive</a>
    <a href="https://app.cristin.no/results/show.jsf?id=2173736" alt="Cristin" class="csl-bib-button">Cristin</a>
    <a href="http://zotero.org/groups/5881554/items/RUAX49K6" alt="Zotero" class="csl-bib-button">Zotero</a>
    <a href="#keywords-article-RUAX49K6" alt="keywords" class="csl-bib-button">Keywords</a>
    <a href="#about-article-RUAX49K6" alt="about_pub" class="csl-bib-button">About</a>
    <a href="#sdg-article-RUAX49K6" alt="sdg" class="csl-bib-button">Sustainable Development Goals</a>
    <a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rse2.367" alt="Unpaywall" class="csl-bib-button">Unpaywall</a>
    <a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rse2.367" alt="EZproxy" class="csl-bib-button">EZproxy</a>
  </div>
  <div id="csl-bib-meta-container-RUAX49K6"></div>
</article>
<div id="csl-bib-meta-RUAX49K6" class="csl-bib-meta">
  <article id="about-article-RUAX49K6" class="about_pub-article">
    <h1>About</h1>
    Mitterwallner et al. conducted a study using camera traps and a machine learning tool called MegaDetector to monitor human and wildlife activities in natural areas. They tested the tool's ability to identify and count humans, animals, and vehicles in over 300,000 images from different regions. The tool showed high accuracy: 96% for animals, 93.8% for people, and 99.3% for vehicles. It also effectively matched manual classifications in long-term studies, proving its reliability for ongoing monitoring. This technology speeds up data processing and ensures consistent results, making it useful for ecological research while respecting privacy rules. The study highlights how technology can help us better understand interactions between humans and wildlife, which is important as human activities in natural areas increase.
  </article>
  <article id="keywords-article-RUAX49K6" class="keywords-article">
    <h1>Keywords</h1>
    Camera Traps, Machine Learning, Object Detection, Human-Wildlife Interactions, Image Classification, Temporal Analysis
  </article>
  <article id="abstract-article-RUAX49K6" class="abstract-article">
    <h1>Scientific abstract</h1>
    As human activities in natural areas increase, understanding human–wildlife interactions is crucial. Big data approaches, like large‐scale camera trap studies, are becoming more relevant for studying these interactions. In addition, open‐source object detection models are rapidly improving and have great potential to enhance the image processing of camera trap data from human and wildlife activities. In this study, we evaluate the performance of the open‐source object detection model MegaDetector in cross‐regional monitoring using camera traps. The performance at detecting and counting humans, animals and vehicles is evaluated by comparing the detection results with manual classifications of more than 300 000 camera trap images from three study regions. Moreover, we investigate structural patterns of misclassification and evaluate the results of the detection model for typical temporal analyses conducted in ecological research. Overall, the accuracy of the detection model was very high with 96.0% accuracy for animals, 93.8% for persons and 99.3% for vehicles. Results reveal systematic patterns in misclassifications that can be automatically identified and removed. In addition, we show that the detection model can be readily used to count people and animals on images with underestimating persons by −0.05, vehicles by −0.01 and animals by −0.01 counts per image. Most importantly, the temporal pattern in a long‐term time series of manually classified human and wildlife activities was highly correlated with classification results of the detection model (Pearson's r = 0.996, p  0.001) and diurnal kernel densities of activities were almost equivalent for manual and automated classification. The results thus prove the overall applicability of the detection model in the image classification process of cross‐regional camera trap studies without further manual intervention. Besides the great acceleration in processing speed, the model is also suitable for long‐term monitoring and allows reproducibility in scientific studies while complying with privacy regulations.
  </article>
  <article id="sdg-article-RUAX49K6" class="sdg-article">
    <h1>Sustainable Development Goals</h1>
    <div class="sdg-container"><div id="sdg15" class="sdg">
        <img src="{{< params subfolder >}}images/sdg/sdg15_en.png" class="image" alt="SDG 15">
        <div class="sdg-overlay">
          <a href="/en/archive/?key=?sdg=15#archive" class="sdg-publication-count"><span>538</span> publications</a>
          <p><a href="https://sdgs.un.org/goals/goal15" class="sdg-read-more">Read More</a></p>
        </div>
      </div></div>
  </article>
  <article id="taxonomy-article-RUAX49K6" class="taxonomy-article">
    <h1>Archive</h1>
    <ul>
      <li>
        <a href="/en/archive/?key=3DCRN523">University of Inland Norway</a>
      </li>
      <li>
        <a href="/en/archive/?key=T77LXH6D">Faculty of Applied Ecology, Agricultural Sciences and Biotechnology</a>
      </li>
      <li>
        <a href="/en/archive/?key=7TRARPE3">Department of Forestry and Wildlife Management</a>
      </li>
      <li>
        <a href="/en/archive/?key=WXLLSUEU">2023</a>
      </li>
      <li>
        <a href="/en/archive/?key=AGMKHRCB">September</a>
      </li>
    </ul>
  </article>
</div>
