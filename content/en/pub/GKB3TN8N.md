---
title: Action Detection for Wildlife Monitoring with Camera Traps Based on Segmentation
  with Filtering of Tracklets (SWIFT) and Mask-Guided Action Recognition (MAROON)
type: pub
encoding: UTF-8

---
<h1>Publication</h1>
<article id="csl-bib-container-GKB3TN8N" class="csl-bib-container">
  <div class="csl-bib-body"> <div class="csl-entry">Schindler, F., Steinhage, V., van Beeck Calkoen, S. T. S., &#38; Heurich, M. (2024). Action Detection for Wildlife Monitoring with Camera Traps Based on Segmentation with Filtering of Tracklets (SWIFT) and Mask-Guided Action Recognition (MAROON). <i>Applied Sciences</i>, <i>14</i>(2), 514. <a href="https://doi.org/10.3390/app14020514">https://doi.org/10.3390/app14020514</a></div> </div>
  <div class="csl-bib-buttons">
    <a href="#taxonomy-article-GKB3TN8N" alt="archive" class="csl-bib-button">Archive</a>
    <a href="https://app.cristin.no/results/show.jsf?id=2270413" alt="Cristin" class="csl-bib-button">Cristin</a>
    <a href="http://zotero.org/groups/5881554/items/GKB3TN8N" alt="Zotero" class="csl-bib-button">Zotero</a>
    <a href="#keywords-article-GKB3TN8N" alt="keywords" class="csl-bib-button">Keywords</a>
    <a href="#about-article-GKB3TN8N" alt="about_pub" class="csl-bib-button">About</a>
    <a href="#sdg-article-GKB3TN8N" alt="sdg" class="csl-bib-button">Sustainable Development Goals</a>
    <a href="https://www.mdpi.com/2076-3417/14/2/514/pdf?version=1704619491" alt="Unpaywall" class="csl-bib-button">Unpaywall</a>
    <a href="https://www.mdpi.com/2076-3417/14/2/514/pdf?version=1704619491" alt="EZproxy" class="csl-bib-button">EZproxy</a>
  </div>
  <div id="csl-bib-meta-container-GKB3TN8N"></div>
</article>
<div id="csl-bib-meta-GKB3TN8N" class="csl-bib-meta">
  <article id="about-article-GKB3TN8N" class="about_pub-article">
    <h1>About</h1>
    Schindler et al. developed a new method to automatically analyze animal behavior in the wild using camera trap videos. Their approach, called SWIFT and MAROON, detects animals and recognizes their actions. SWIFT helps track animals in videos, while MAROON uses detailed masks and a triple-stream network to identify actions more accurately, even when multiple animals are present. Tested on videos of deer recorded both day and night, MAROON improved action recognition accuracy by 10 percentage points compared to other methods, achieving 69.16% accuracy on one dataset. This system can significantly reduce the manual work for ecologists and provide consistent, valuable insights into animal behavior.
  </article>
  <article id="keywords-article-GKB3TN8N" class="keywords-article">
    <h1>Keywords</h1>
    Wildlife Monitoring, Camera Traps, Action Detection, Animal Behavior, SWIFT, MAROON
  </article>
  <article id="abstract-article-GKB3TN8N" class="abstract-article">
    <h1>Scientific abstract</h1>
    Behavioral analysis of animals in the wild plays an important role for ecological research and conservation and has been mostly performed by researchers. We introduce an action detection approach that automates this process by detecting animals and performing action recognition on the detected animals in camera trap videos. Our action detection approach is based on SWIFT (segmentation with filtering of tracklets), which we have already shown to successfully detect and track animals in wildlife videos, and MAROON (mask-guided action recognition), an action recognition network that we are introducing here. The basic ideas of MAROON are the exploitation of the instance masks detected by SWIFT and a triple-stream network. The instance masks enable more accurate action recognition, especially if multiple animals appear in a video at the same time. The triple-stream approach extracts features for the motion and appearance of the animal. We evaluate the quality of our action recognition on two self-generated datasets, from an animal enclosure and from the wild. These datasets contain videos of red deer, fallow deer and roe deer, recorded both during the day and night. MAROON improves the action recognition accuracy compared to other state-of-the-art approaches by an average of 10 percentage points on all analyzed datasets and achieves an accuracy of 69.16% on the Rolandseck Daylight dataset, in which 11 different action classes occur. Our action detection system makes it possible todrasticallyreduce the manual work of ecologists and at the same time gain new insights through standardized results.
  </article>
  <article id="sdg-article-GKB3TN8N" class="sdg-article">
    <h1>Sustainable Development Goals</h1>
    <div class="sdg-container"><div id="sdg15" class="sdg">
        <img src="{{< params subfolder >}}images/sdg/sdg15_en.png" class="image" alt="SDG 15">
        <div class="sdg-overlay">
          <a href="{{< params subfolder >}}en/archive/?sdg=15#archive" class="sdg-publication-count"><span>538</span> publications</a>
          <p><a href="https://sdgs.un.org/goals/goal15" class="sdg-read-more">Read More</a></p>
        </div>
      </div></div>
  </article>
  <article id="taxonomy-article-GKB3TN8N" class="taxonomy-article">
    <h1>Archive</h1>
    <ul>
      <li><a href="{{< params subfolder >}}en/archive/?key=3DCRN523">University of Inland Norway</a></li>
      <li><a href="{{< params subfolder >}}en/archive/?key=T77LXH6D">Faculty of Applied Ecology, Agricultural Sciences and Biotechnology</a></li>
      <li><a href="{{< params subfolder >}}en/archive/?key=7TRARPE3">Department of Forestry and Wildlife Management</a></li>
      <li><a href="{{< params subfolder >}}en/archive/?key=A4XX8HDP">2024</a></li>
      <li><a href="{{< params subfolder >}}en/archive/?key=2IXDX4YB">May</a></li>
    </ul>
  </article>
</div>
